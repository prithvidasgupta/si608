{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T13:37:04.220841Z",
     "start_time": "2023-12-14T13:36:39.661400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "dataset_tuples = []\n",
    "\n",
    "\n",
    "with open('../all_nodes.pkl', 'rb') as f:\n",
    "    nodes = pickle.load(f)\n",
    "\n",
    "with gzip.open('../data/2017-2023/clickstream-enwiki-2019-01.tsv.gz', 'rt', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if parts[0] in nodes and parts[1] in nodes:\n",
    "            dataset_tuples.append((parts[0],parts[1],parts[2],int(parts[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T13:37:14.667271Z",
     "start_time": "2023-12-14T13:37:04.809552Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset_tuples, columns=['prev', 'curr', 'type' ,'clicks'])\n",
    "\n",
    "def change_prev(cell):\n",
    "    if cell in ['other-search', 'other-other', 'other-external', 'other-empty',\n",
    "       'other-internal']:\n",
    "        return '_excess_external_adjust_'\n",
    "    return cell\n",
    "df['prev'] = df['prev'].apply(change_prev)\n",
    "\n",
    "outflow = df.groupby('prev')['clicks'].sum().reset_index()\n",
    "inflow = df.groupby('curr')['clicks'].sum().reset_index()\n",
    "\n",
    "t = inflow.merge(outflow, how='left', left_on=['curr'], right_on=['prev'], suffixes=['_inflow', '_outflow']).fillna(0)\n",
    "t['delta']=t['clicks_inflow']-t['clicks_outflow']\n",
    "net_counts = []\n",
    "for page, c_i, c_o in zip(t['prev'], t['clicks_inflow'], t['clicks_outflow']):\n",
    "    if c_i < c_o:\n",
    "        net_counts.append((page,c_o))\n",
    "    else:\n",
    "        net_counts.append((page, c_i))\n",
    "\n",
    "t = pd.DataFrame(net_counts, columns=['page', 'total'])\n",
    "\n",
    "probability_df = df.merge(t, how='left', left_on=['prev'], right_on=['page'])\n",
    "probability_df=probability_df.fillna(df[df['prev']=='_excess_external_adjust_']['clicks'].sum())\n",
    "probability_df['transition_probability'] = probability_df['clicks']/probability_df['total']\n",
    "idx_page = list(set([*probability_df['curr'].unique(), *probability_df['prev'].unique()]))\n",
    "page_idx = {page:i for i, page in enumerate(idx_page)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T13:37:22.625434Z",
     "start_time": "2023-12-14T13:37:14.667466Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6484995/6484995 [00:05<00:00, 1245688.83it/s]\n",
      "100%|██████████| 6484995/6484995 [00:02<00:00, 2403481.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def change_to_idx(cell):\n",
    "    return page_idx[cell]\n",
    "\n",
    "probability_df['i'] = probability_df['prev'].progress_apply(change_to_idx)\n",
    "probability_df['j'] = probability_df['curr'].progress_apply(change_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T13:37:22.629607Z",
     "start_time": "2023-12-14T13:37:22.625529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "float"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(1-3.341394e-09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T13:37:22.875857Z",
     "start_time": "2023-12-14T13:37:22.630268Z"
    }
   },
   "outputs": [],
   "source": [
    "pad_df = probability_df.groupby('i')['transition_probability'].sum().sort_values(ascending=False).reset_index()\n",
    "pad_df['padding'] = [str(1-p) for p in pad_df['transition_probability']]\n",
    "s = set(pad_df['i'].unique())\n",
    "extra = [i for i in range(len(page_idx)) if i not in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                                  prev                    curr   type  clicks  \\\n4003316             Frasier_(season_8)            Derek_Jacobi   link      42   \n5071240  Cardiopulmonary_resuscitation        Lazarus_syndrome   link     142   \n1817204                      Main_Page         Boyd_Coddington  other      43   \n2816760            Inductive_reasoning  Mathematical_induction   link     279   \n1771643               Night_Train_Lane           Detroit_Lions   link      28   \n\n                                  page        total  transition_probability  \\\n4003316             Frasier_(season_8)       4720.0            8.898305e-03   \n5071240  Cardiopulmonary_resuscitation      55062.0            2.578911e-03   \n1817204                      Main_Page  513712269.0            8.370444e-08   \n2816760            Inductive_reasoning      52831.0            5.280990e-03   \n1771643               Night_Train_Lane       7538.0            3.714513e-03   \n\n              i       j  \n4003316  101457   36145  \n5071240   67596  234249  \n1817204   60591  231886  \n2816760  166647  128361  \n1771643  179233   81494  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prev</th>\n      <th>curr</th>\n      <th>type</th>\n      <th>clicks</th>\n      <th>page</th>\n      <th>total</th>\n      <th>transition_probability</th>\n      <th>i</th>\n      <th>j</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4003316</th>\n      <td>Frasier_(season_8)</td>\n      <td>Derek_Jacobi</td>\n      <td>link</td>\n      <td>42</td>\n      <td>Frasier_(season_8)</td>\n      <td>4720.0</td>\n      <td>8.898305e-03</td>\n      <td>101457</td>\n      <td>36145</td>\n    </tr>\n    <tr>\n      <th>5071240</th>\n      <td>Cardiopulmonary_resuscitation</td>\n      <td>Lazarus_syndrome</td>\n      <td>link</td>\n      <td>142</td>\n      <td>Cardiopulmonary_resuscitation</td>\n      <td>55062.0</td>\n      <td>2.578911e-03</td>\n      <td>67596</td>\n      <td>234249</td>\n    </tr>\n    <tr>\n      <th>1817204</th>\n      <td>Main_Page</td>\n      <td>Boyd_Coddington</td>\n      <td>other</td>\n      <td>43</td>\n      <td>Main_Page</td>\n      <td>513712269.0</td>\n      <td>8.370444e-08</td>\n      <td>60591</td>\n      <td>231886</td>\n    </tr>\n    <tr>\n      <th>2816760</th>\n      <td>Inductive_reasoning</td>\n      <td>Mathematical_induction</td>\n      <td>link</td>\n      <td>279</td>\n      <td>Inductive_reasoning</td>\n      <td>52831.0</td>\n      <td>5.280990e-03</td>\n      <td>166647</td>\n      <td>128361</td>\n    </tr>\n    <tr>\n      <th>1771643</th>\n      <td>Night_Train_Lane</td>\n      <td>Detroit_Lions</td>\n      <td>link</td>\n      <td>28</td>\n      <td>Night_Train_Lane</td>\n      <td>7538.0</td>\n      <td>3.714513e-03</td>\n      <td>179233</td>\n      <td>81494</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_df.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:39:00.310651Z",
     "start_time": "2023-12-14T13:39:00.170298Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T16:50:02.089559Z",
     "start_time": "2023-12-13T16:49:55.534004Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "weight =[]\n",
    "adj = []\n",
    "row = []\n",
    "col = []\n",
    "\n",
    "for i,j, prob in zip(probability_df['i'],probability_df['j'],probability_df['transition_probability']):\n",
    "    row.append(i)\n",
    "    col.append(j)\n",
    "    weight.append(prob)\n",
    "    adj.append(1)\n",
    "\n",
    "for i,prob in zip(pad_df['i'], pad_df['padding']):\n",
    "    if float(prob)>0:\n",
    "        row.append(i)\n",
    "        col.append(page_idx['_excess_external_adjust_'])\n",
    "        weight.append(float(prob))\n",
    "        adj.append(1)\n",
    "\n",
    "for i in extra:\n",
    "        row.append(i)\n",
    "        col.append(page_idx['_excess_external_adjust_'])\n",
    "        weight.append(1)\n",
    "        adj.append(1)\n",
    "\n",
    "adj_matrix = csr_matrix((adj, (row,col)), shape=(len(page_idx), len(page_idx)))\n",
    "wt_matrix = csr_matrix((weight, (row,col)), shape=(len(page_idx), len(page_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Write to disk\n",
    "adj_matrix_df = pd.DataFrame.sparse.from_spmatrix(adj_matrix)\n",
    "wt_matrix_df = pd.DataFrame.sparse.from_spmatrix(wt_matrix)\n",
    "adj_matrix_df.to_parquet(\"../output/random_walks/adj_matrix/2019-01_sept11.parquet\")\n",
    "wt_matrix_df.to_parquet(\"../output/random_walks/wt_matrix/2019-01_sept11.parquet\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T16:50:06.821676Z",
     "start_time": "2023-12-13T16:50:02.109472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.0019243227275166465"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Main_Page'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'1999_Manitoba_general_election'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0.0019243227275165592"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Twin_Towers'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'September_11_attacks'"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.sparse.linalg import eigs\n",
    "import numpy as np\n",
    "pdf = [1/len(idx_page) for i in range(len(idx_page))] # initial probability uniform\n",
    "egv, egvect= eigs(wt_matrix, v0=pdf)\n",
    "prv, prank = egv[0], abs(egvect[:,0])\n",
    "display(prank[page_idx['September_11_attacks']], idx_page[np.argmax(prank)], idx_page[np.argmin(prank)])\n",
    "\n",
    "# Write to disk\n",
    "# page_idx == index in resulting DataFrame\n",
    "eig_cents = pd.DataFrame(prank, columns=[\"eig_cent\"])\n",
    "page_names = pd.DataFrame(idx_page, columns=[\"page\"])\n",
    "joined_pages_eigs = page_names.join(eig_cents)\n",
    "joined_pages_eigs.to_csv(\"../output/random_walks/uniform_prob/2019-01_sept11_uniform.csv\")\n",
    "\n",
    "pdf = np.zeros(len(page_idx))\n",
    "pdf[page_idx['September_11_attacks']] = 1 # initial probability not uniform\n",
    "egv, egvect= eigs(wt_matrix, v0=pdf)\n",
    "prv, prank = egv[0], abs(egvect[:,0])\n",
    "display(prank[page_idx['September_11_attacks']], idx_page[np.argmax(prank)], idx_page[np.argmin(prank)])\n",
    "\n",
    "# Write to disk\n",
    "# page_idx == index in resulting DataFrame\n",
    "eig_cents = pd.DataFrame(prank, columns=[\"eig_cent\"])\n",
    "page_names = pd.DataFrame(idx_page, columns=[\"page\"])\n",
    "joined_pages_eigs = page_names.join(eig_cents)\n",
    "joined_pages_eigs.to_csv(\"../output/random_walks/not_uniform_prob/2019-01_sept11_~uniform.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, ..., 1, 1, 1])"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adj_matrix.data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:57:11.457194Z",
     "start_time": "2023-12-13T16:57:11.437380Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.00688675, 0.00480979, 0.00174902, ..., 0.01660819, 0.00491228,\n       0.01473684])"
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wt_matrix.data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:57:44.699819Z",
     "start_time": "2023-12-13T16:57:44.682161Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
